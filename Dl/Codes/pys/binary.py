import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_ datasets as tfds
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from mlxtend.plotting import plot_confusion_matrix
from sklearn import metrics
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
from tqdm.notebook import tqdm
import warnings
warnings. filterwarnings ("ignore")

train_data, validation_data, test_data = tfds.load(
name="imdb_reviews",
split=('train[:60%]', ‘train[60%:]', ‘test'),
as_supervised=True) 

train_examples_batch, train_labels_batch = next(iter(train_data.batch(10) ) )
train_labels_batch 

embedding = "https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2"
hub_layer hub.KerasLayer(embedding, input_shape=[],
dtype=tf.string, trainable=True) 

model = tf.keras.Sequential ([
hub_layer,
tf.keras.layers.Dense(32, activation='relu', name='hidden-layer-2'),
tf.keras.layers.Dense(16, activation='relu', name='hidden-layer-3'),
tf.keras.layers.Dense(1, name='output-layer' )
}) 

model.summary() 

model.compile(optimizer='adam' ,
loss='binary_crossentropy' ,
metrics=[ ‘accuracy’ ]) 

history = model.fit(train_data.shuffle(10000).batch(512),
epochs=5,
validation_data=validation_data.batch(512),
verbose=1) 

results = model.evaluate(test_data.batch(512), verbose=2)
for name, value in zip(model.metrics_names, results):
print("%s: %.3f" % (name, value)) 

pd.DataFrame(history.history) .plot(figsize=(10,7))
plt.title("Metrics Graph")
plt.show() 

texts = []
true_labels = []
for text, label in test_data:
texts.append(text.numpy() )
true_labels.append(label.numpy() )
texts = np.array(texts)
true_labels = np.array(true_labels) 

predicted_probs = model.predict(texts) 

predicted_labels = (predicted_probs > 0.5).astype(int) 

cm = metrics.confusion_matrix(true_labels, predicted_labels)
plot_confusion_matrix(cm, class_names=[ ‘Negative’, "Positive" ]) 

plt.title("Confusion Matrix")
plt.show()